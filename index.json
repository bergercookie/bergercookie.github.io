[{"content":"This is part two of my articles on the Albert launcher. Read the first part here\nThis post outlines 5 common issues that I had been facing during my day-to-day development and the solutions that I came up with via 5 Albert plugins respectively.\nTL;DR The purpose of these plugins is a) to avoid context-switching and b) have a single tool with a single interface instead X different tools with their own corresponding interfaces.\nAll are hosted under the same github repo: awesome-albert-plugins on github and here are links to each one of the plugins that I\u0026rsquo;ll be discussing below:\n TLDR pages Scratchpad Googler-enabled plugins google_translate | words Saxophone  üîé Looking up command / tool usage instructions During your daily routine, you\u0026rsquo;ll want to look up how to call a certain tool, say recursively compress the contents of a directory using bzip, or create a new virtual environment to work in using Poetry. You may either haven\u0026rsquo;t used these tools at all, or are not able to recall the exact flags for the task at hand. Two common ways of dealing with this are:\n Google it! You\u0026rsquo;ll then most probably want to move to a stackoverflow answer, copy-paste a command that seems to solve your issue, and then adjust it accordingly. This involves a few browser redirections (at least google.com -\u0026gt; stackoverflow.com), as well as the overhead of selecting the right post and finding the right answer to your question. Read the manpage / help page of the tool, if it has one. This resource may be more rigorous but it will also take more time to find the documentation section that you\u0026rsquo;re interested in.  A more effective approach to this would be to use a software such as tldr, bro pages, or cheat. More specifically, tldr allows you to look up the most common usecases for a wide variety of tools, all without leaving your command line. It\u0026rsquo;s also easily extensible, allowing you to add more examples to specific tools, or adding documentation and examples for new tools.\nHowever even in this case you have to switch context from what you\u0026rsquo;re currently doing; If you\u0026rsquo;re working e.g., on VS Code, you have to start a new terminal, look up the tool that you want, copy the usecase that you\u0026rsquo;re interested in, paste and modify it accordingly. We can do better than that.\nEnter the tldr_pages albert plugin\nIt allows you to do fuzzy autocompletion-enabled search on any tldr command and copy its content on ENTER. In addition you can also quickly navigate to the appropriate webpage if you want to read more about the tool at hand, or fall back to a google search if that\u0026rsquo;s not good enough.\nüìì Taking notes instantly - Refactor later Whether I\u0026rsquo;m either reading articles on Wikipedia, watching videos on YouTube, or doing pretty much anything on the computer that involves a bit of learning, I like to take notes. A good approach to this would be to split your screen vertically and have the browser on one side and your favorite editor on the other. Then you would create a new text file for every new item you \u0026rsquo;re studying, add a title, then add your notes or copy paste accordingly.\nThis involves a few steps in the process that can be improved:\n  At that particular moment I don\u0026rsquo;t want to have to:\n create a new file think of where to place it in my hierarchy of notes add the appropriate title add structure to the note that I\u0026rsquo;m taking  I\u0026rsquo;d rather spend this time just recording my thoughts and reading through the actual Wikipedia page.\n  I don\u0026rsquo;t want to switch between reading the resource and recording my notes.\n  To deal with this, I\u0026rsquo;m using a very simple plugin called scratchpad.\nIts logic is super simple. You write some text to it and it writes it to a file; The same file all the time. You specify the path to that file the first time you trigger the plugin.\nEach text is saved there and by default it\u0026rsquo;s formatted with a maximum width of 80 characters to assist in later potential reformatting. A blank line is inserted between successive entries to the file and it allows to add a separator if you start adding notes about a new subject\nThe plugin gets triggered either explicitly using s\u0026lt;space\u0026gt; or automatically if your Albert query is longer than 5 words. This process allows you to record anything you want and then sort them out later (i.e., place them into separate files, structure them better, etc.).\nHere\u0026rsquo;s how your scratchpad looks like after a bunch of additions from two different pages:\nüîó Adding Links during Text Editing Another common issue is inserting links to other pages or documentation while you\u0026rsquo;re creating or editing an existing page. That may be a markdown document for your GitHub repo, a Confluence page or Jira issue in your day job or a report you\u0026rsquo;re righting in LibreOffice. Most times you\u0026rsquo;d have to stop what you\u0026rsquo;re doing, launch your browser, search on google the thing you\u0026rsquo;re interested in and navigate to it, and finally, copy the link displayed in your browser prompt\nAgain, as with the previous issues discussed this approach takes too much time and makes you context switch from the thing you\u0026rsquo;re currently doing.\nInstead use googler or even better one of the many googler-enabled plugins in the awesome-albert-plugins repo. You can basically use the create_googler_plugins.py script after you\u0026rsquo;ve downloaded the repository to several Albert plugins, each one responsible for searching in a single website. So for example, you can use the trigger gg to search and get results for Google, or gh to search on GitHub or imdb to search on IMDB.\nHere\u0026rsquo;s how it looks:\n| | | | | |\nUsing one of these plugins, for example the one that searches on Google, you can search for and copy the link to the resource you\u0026rsquo;re interested in without leaving the document you\u0026rsquo;re currently editing.\nüàØ Translating Text Again, same premise as in the previous cases. You\u0026rsquo;re reading an article or watching a movie and you don\u0026rsquo;t know a word or a phrase written there. It\u0026rsquo;s too much of a hassle to open a new browser tab, go to Google Translate to do the translation.\nUse the google_translate plugin in conjunction with the word plugin.\n| | |\nThe google_translate plugin will translate the word from the source language to the destination language while you can also use the auto source to enable autodetection.\nThe word plugin on the other hand will give you the definition of the given word along with synonyms and antonyms.\nüé∑ Playing Radio Streams This last plugin allows you to play and alternate between different stations easily and without jumping around different webpages to do so.\nIt\u0026rsquo;s called saxophone, and this is how its interface looks.\nThere\u0026rsquo;s a bunch of preconfigured radios, specified in the config/saxophone.json file. You can easily extend that list to include your preferences as well.\nUnder the hood it uses the VLC Remote-Control Interface.\n","permalink":"https://bergercookie.dev/post/5-albert-plugins-to-5-common-problems/","summary":"This is part two of my articles on the Albert launcher. Read the first part here\nThis post outlines 5 common issues that I had been facing during my day-to-day development and the solutions that I came up with via 5 Albert plugins respectively.\nTL;DR The purpose of these plugins is a) to avoid context-switching and b) have a single tool with a single interface instead X different tools with their own corresponding interfaces.","title":"5 Albert Plugins to 5 Workflow Issues"},{"content":"Behavioral psychology and the works of Kanheman and Tversky has been a hobby of mine for quite some time now. Same goes for podcasts like Freakonomics and Choiceology. Given that, it\u0026rsquo;s only natural to come across the Farmam Street blog by Shane Parish and more specifically, the article on Mental Models.\nMental models are heuristics that simplify reality and help you navigate the world around you. For example they may help you make better decisions, explain other people\u0026rsquo;s actions, close deals at work or study more efficiently for an exam.\nThe Farmam Street Post I linked above provides quite an detailed introduction to many mental models that you can use on an everyday basis. However because of that breadth (it lists over 70 different models along with a complete description for each one) it\u0026rsquo;s quite hard to read through all of them.\nInstead, I created a mind map for the same content which puts all the models into groups and in a single page. Have a look below, and let me know what you think:\n ","permalink":"https://bergercookie.dev/post/mental-models/","summary":"Behavioral psychology and the works of Kanheman and Tversky has been a hobby of mine for quite some time now. Same goes for podcasts like Freakonomics and Choiceology. Given that, it\u0026rsquo;s only natural to come across the Farmam Street blog by Shane Parish and more specifically, the article on Mental Models.\nMental models are heuristics that simplify reality and help you navigate the world around you. For example they may help you make better decisions, explain other people\u0026rsquo;s actions, close deals at work or study more efficiently for an exam.","title":"A Mind Map of Mental Models"},{"content":"This is a writeup of my first assignment for the \u0026ldquo;Computer Vision II: Applications\u0026rdquo; course.\nFor this assignment, I\u0026rsquo;m given an image of a girl, and I have to add 2 new features to it:\n Apply lipstick Applying blush  Here\u0026rsquo;s the original image that we\u0026rsquo;ll be working on:\n  Initialisation actions We first load and initialise all the necessary modules and objects that we\u0026rsquo;ll need\n1 2 3 4 5 6 7 8   # various imports  import cv2, sys, dlib, time, math  import numpy as np  import matplotlib.pyplot as plt  from pathlib import Path  from typing import Tuple  from cv_helpers import *  from helpers import *   Note that cv_helpers and helpers are personal modules, mainly for debugging / visualisation purposes.\nWe then load the dlib 68 point face detector:\n1 2 3 4   path = Path(__file__).absolute().parent  predictor_path = path / \u0026#34;shape_predictor_68_face_landmarks.dat\u0026#34;  faceDetector = dlib.get_frontal_face_detector()  landmarkDetector = dlib.shape_predictor(str(predictor_path))   Notice that we\u0026rsquo;re taking the absolute path to the trained predictor; That\u0026rsquo;s the correct way of opening files since this allows you to run the application regardless your working directory. We then load the image via the cv2.imread function, taking care of the BGR -\u0026gt; RGB conversion since OpenCV uses the BGR format and finally we detect the face landmarks:\n1 2 3   im = cv2.imread(str(path / \u0026#34;girl-no-makeup.jpg\u0026#34;))  imDlib = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)  landmarks = fbc.getLandmarks(faceDetector, landmarkDetector, im)   To verify that we\u0026rsquo;ve loaded the image and we\u0026rsquo;ve detected the required features succesfully, we can plot the original image, along with it superimposed by the detected features. To do that, we\u0026rsquo;ll make use of the following helper functions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57   def mark_points_in_img(img: np.ndarray, points, inplace=True) -\u0026gt; np.ndarray:  \u0026#34;\u0026#34;\u0026#34; Each one of the points in the `points` sequence is marked by a circle and its corresponding index in text. \u0026#34;\u0026#34;\u0026#34;   if not inplace:  img = img.copy()   for i, la in enumerate(points):  cv2.circle(img, la, radius=2, color=(0, 255, 0))  cv2.putText(img, str(i), la, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)   return img   def get_num_rows_cols(size: int, max_cols: int = 2) -\u0026gt; Tuple[int, int]:  \u0026#34;\u0026#34;\u0026#34;Format `size` elements into an xy grid. \u0026gt;\u0026gt;\u0026gt; get_num_rows_cols(2, max_cols=4) (1, 2) \u0026gt;\u0026gt;\u0026gt; get_num_rows_cols(4, max_cols=4) (1, 4) \u0026gt;\u0026gt;\u0026gt; get_num_rows_cols(5, max_cols=4) (2, 4) \u0026gt;\u0026gt;\u0026gt; get_num_rows_cols(10, max_cols=4) (3, 4) \u0026gt;\u0026gt;\u0026gt; get_num_rows_cols(6, max_cols=2) (3, 2) \u0026#34;\u0026#34;\u0026#34;   ncols = min(size, max_cols)  if size % ncols == 0:  nrows = size // ncols  else:  nrows = int(np.ceil(size / ncols))   return nrows, ncols   def plt_imshow(*imgs, is_bgr=False):  \u0026#34;\u0026#34;\u0026#34; Render the given images in matplotlib. The function will automatically arrange them in a grid layout if they are more than 2. \u0026#34;\u0026#34;\u0026#34;   if is_bgr:  imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in imgs]   if not imgs:  return   n = len(imgs)  rows, cols = get_num_rows_cols(n, max_cols=3)  fig = plt.figure(figsize=(8, 8))  for i, img in enumerate(imgs):  fig.add_subplot(rows, cols, i + 1)  plt.imshow(img)  plt.show()   And now to put these functions to good use:\n1 2 3   # Copy the image - keep the original intact  im1 = mark_points_in_img(imDlib, landmarks, inplace=False)  plt_imshow(imDlib, im1)   And here\u0026rsquo;s the result:\nApplying lipstick To apply lipstick to the girl in the image, we follow the steps below:\n Identify the list of landmarks that comprise the lips area as well as the area that corresponds to the mouth cavity (in case the person has slightly opened their mouth). Initialise a new black image, that has the same size as the original one, and has a solid red color at the lips. Slightly blur the latter, in order to soften its edges and hide potential defects in the end result Finally do an alpha-blend of the two images (original girl image, lipsticks only image) giving a higher weight to the girl  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24   im_lipstick = im.copy()   # gather the indexes of the outer and inner lip cavities  lips_outer_idx = range(48, 60)  lips_outer_landmarks = [landmarks[i] for i in lips_outer_idx]  lips_inner_idx = range(60, 67)  lips_inner_landmarks = [landmarks[i] for i in lips_inner_idx]   # create lipstick-only area  im_lipstick_only = np.zeros(im_lipstick.shape, dtype=im_lipstick.dtype)  cv2.fillPoly(  im_lipstick_only, np.array([lips_outer_landmarks], np.int32), color=(0, 0, 255),  )  cv2.fillConvexPoly( # mouth cavity  im_lipstick_only, np.array(lips_inner_landmarks, np.int32), color=(0, 0, 0),  )   # do a minor blur to sotften the edges  im_lipstick_only = cv2.blur(im_lipstick_only, (5, 5))   alpha = 0.8  im_lipstick = cv2.addWeighted(im, alpha, im_lipstick_only, 1 - alpha, 0.0)   plt_imshow(im, im_lipstick, is_bgr=True)   Here\u0026rsquo;s how the girl looks after having applied the lipstick:\nApplying blush Let\u0026rsquo;s start where we left off at the previous section,\n1 2   im = im_lipstick  im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)   Our goal is to use a second image, let\u0026rsquo;s call it im2 which has intense blush characteristics, and try to somehow blend it with our original image, im. To do that, we first normalise both im and im2 to a specifc size and using specific features and distances as reference for the normalisation. For this, as in the first step, we will use the fbc.normalizeImagesAndLandmarks function provided in the course material. However, instead of using the corners of the eyes, as the points for normalisation , we will instead use the distance between the cheek bones. Thus we add an extra class to encode this extra information, and we change the signature of the fbc.normalizeImagesAndLandmarks function as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22   class PointsForNormalisation:  loc0: Tuple[int, int] = (0, 0)  loc1: Tuple[int, int] = (0, 0)  new_loc0: Tuple[int, int] = (0, 0)  new_loc1: Tuple[int, int] = (0, 0)   def normalizeImagesAndLandmarks(outSize: Tuple[int, int], imIn, pointsIn: np.ndarray,  points_for_normalisation:  Optional[PointsForNormalisation]=None):   ...  if points_for_normalisation:  ps = points_for_normalisation  loc0 = ps.loc0  loc1 = ps.loc1   new_loc0 = ps.new_loc0  new_loc1 = ps.new_loc1   else:  # business as usual  # use corners of eyes   We then execute the normalisation as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31   dst_shape = (600, 600)  landmarks_arr = np.array(landmarks)   def get_ps_for_cheeks(landmarks: np.ndarray, w, h) -\u0026gt; fbc.PointsForNormalisation:  ps = fbc.PointsForNormalisation()  ps.loc0 = landmarks[2]  ps.loc1 = landmarks[14]   ps.new_loc0 = (np.int(0.15 * w), np.int(h / 2))  ps.new_loc1 = (np.int(0.85 * w), np.int(h / 2))   return ps   im, landmarks_arr = fbc.normalizeImagesAndLandmarks(dst_shape, im, landmarks_arr,  get_ps_for_cheeks(landmarks_arr,  dst_shape[0],  dst_shape[1]))  landmarks = [(la[0], la[1]) for la in landmarks_arr]  assert im.shape[:-1] == dst_shape   # load, find landmarks in and reshape second image  im2_orig = cv2.imread(str(path / \u0026#34;makeup2.jpeg\u0026#34;))  im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)  landmarks2 = fbc.getLandmarks(faceDetector, landmarkDetector, im2)  landmarks_arr2 = np.array(landmarks2)  im2, landmarks_arr2 = fbc.normalizeImagesAndLandmarks(dst_shape, im2, landmarks_arr2,  get_ps_for_cheeks(landmarks_arr2,  dst_shape[0],  dst_shape[1]))  landmarks2 = [(la[0], la[1]) for la in landmarks_arr2]  assert im2.shape[:-1] == dst_shape   Let\u0026rsquo;s verify that the normalisation works as expected: 1 2 3 4 5 6   plt_imshow(  cv2.cvtColor(im_lipstick, cv2.COLOR_BGR2RGB),  cv2.cvtColor(im2_orig, cv2.COLOR_BGR2RGB),  im,  im2,  )  \nInitially I thought that doing delaunay triangulation and then warping the triangles around the cheeks area would solve it, however, as demonstrated in the course materials, because of the different texture and skin color, this doesn\u0026rsquo;t work and the end result doesn\u0026rsquo;t look good. I then decided to opt for two seamless cloning operations, one for each cheek, and circular patches in the cheeks as the mask.\nHere\u0026rsquo;s how I finally implemented it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28   # Find the center of each cheek - that\u0026#39;s where the kernel of the blush will be  def get_cheek_loc(a: tuple, b: tuple):  # return (a[0] + b[0])//2, (a[1] + b[1])//2  xdist = b[0] - a[0]  ydist = b[1] - a[1]   x = int(a[0] + 1 / 3 * xdist)  y = int(a[1] + 1 / 3 * ydist)   return x, y   cheek1_xy = get_cheek_loc(landmarks[2], landmarks[29])  cheek2_xy = get_cheek_loc(landmarks[14], landmarks[29])   # create the masks  mask1 = np.zeros(im.shape, im.dtype)  mask2 = mask1.copy()  cv2.circle(mask1, cheek1_xy, radius=50, color=(255, 255, 255), thickness=cv2.FILLED)  cv2.circle(mask2, cheek2_xy, radius=50, color=(255, 255, 255), thickness=cv2.FILLED)   # create a copy for visualisation purposes - show where we copied pixels from  im2_ = im2.copy()  cv2.circle(im2_, cheek1_xy, radius=50, color=(255, 255, 255))  cv2.circle(im2_, cheek2_xy, radius=50, color=(255, 255, 255))   dst = cv2.seamlessClone(np.uint8(im2), im, mask1, cheek1_xy, cv2.NORMAL_CLONE)  dst = cv2.seamlessClone(np.uint8(im2), dst, mask2, cheek2_xy, cv2.NORMAL_CLONE)  plt_imshow(im, im2_, dst)   Here\u0026rsquo;s how the final image looks like:\nAs you can see, the seamless cloning operations brought a few inaccuracies from im2 however the blush itself is visible and almost natural.\nNotes You may need to do a few iterations selecting the image to clone from to find one that looks as you expect. Alternatively if you found one that you want to use but the blush is not that intense, consider editing it in a software like gimp to increase it (e.g., by adding a new layer, painting in it, and setting its opacity to ~60%)\nEven though I demonstrated a roughly straight line between the start of this task and the end result, that really wasn\u0026rsquo;t the case during experimentation. Operations that you think make sense for a certain task may not, and you may need to backtrack, think the problem differently, read the course material a few more times, or even delete the code that you were writing for the whole day. Might not obvious at that point in time, but this is all part of the learning process. As you gain experience in this subject you\u0026rsquo;ll start getting a sense of what operations should be done for each particular class of problems and hopefully the whole process will start getting smoother.\n","permalink":"https://bergercookie.dev/post/opencv-project1/","summary":"This is a writeup of my first assignment for the \u0026ldquo;Computer Vision II: Applications\u0026rdquo; course.\nFor this assignment, I\u0026rsquo;m given an image of a girl, and I have to add 2 new features to it:\n Apply lipstick Applying blush  Here\u0026rsquo;s the original image that we\u0026rsquo;ll be working on:\n  Initialisation actions We first load and initialise all the necessary modules and objects that we\u0026rsquo;ll need\n1 2 3 4 5 6 7 8   # various imports  import cv2, sys, dlib, time, math  import numpy as np  import matplotlib.","title":"Cracking the OpenCV Applications Course - Project 1"},{"content":"I recently got my hands on two books that talk about the Linux Kernel. These are the Linux Kernel in a Nutshell, and LINUX Core Kernel Commentary.\nThese books are rather old, so some bits are outdated, but don\u0026rsquo;t let this fact discourage you. Both can be super useful and can definitely help you learn more about how the Kernel works.\nLinux Kernel In a Nutshell The first one is a classic. Written by Greg Kroah-Hartman, one of the core Linux Kernel developers (who also took over as the lead of the project while Torvalds was away ), It doesn\u0026rsquo;t require any prior knowledge of Linux (or even programming) and guides the reader step by step into the process of building and configuring their kernel from source. Some of its subjects include:\n How to get the Kernel sources. How to use kconfig. A short explanation into the most important kconfig flags as well as heuristics for when to enable each of them. Miscellaneous tools that help in building, maintaining and patching your kernel.  What\u0026rsquo;s interesting about this book is that, it will give a short introduction into each one of the technologies and hardware and the Linux kernel flags for enabling/disabling support for them. Do you know what SAMBA or SELinux is? Perhaps confused as to what the lpj or the cachesize CPU options are during kernel configuration? This book will answer these questions.\nThis way you can learn about a bunch of components in just 20 pages (see Chapter 8: Configuration Recipes) without having to jump to different webpages and having to cross reference the validity of what\u0026rsquo;s you\u0026rsquo;re reading. Here are some example topics:\n PCI Devices  SATA, SCSI and IDE Disk Controllers USB Devices Networking   Filesystems  Types of supported filesystems (FAT, CIFS, ZFS) RAID   CPUs  Linux Core Kernel Commentary While the Linux Kernel in a nutshell is a rather lightweight read, this is definitely not the case with the Linux Core Kernel Commentary. Counting 575 of awkward landscape A4 pages it includes large chunks of code along with separate pages with line-by-line explanation of the code.\nWhat you will learn from it:\n  How to read assembly code\nEven if you are unfamiliar going through assembly code, the book does a pretty good job at explaining how each one of the instructions work as well as offer alternative implementations of what\u0026rsquo;s you are currently reading in C code.\n  Understand concepts like System Calls, Interrupts, and Virtual Memory. On top of that it shows you the actual code used in the kernel that implements these. The latter can be so helpful. It shifts the reader\u0026rsquo;s goal from \u0026ldquo;Let\u0026rsquo;s have a theoretical discussion about X\u0026rdquo; to \u0026ldquo;Let\u0026rsquo;s get down to the nitty-gritty details of how X is being done in a real-life production system\u0026rdquo;.\n  How Linux (well, at least its 2.3.12 version) actually works.\n  The content can be broken up into two parts. The first half contains the raw contents of C and Assembly files from the kernel in a two-column format. The second half contains the corresponding commentary for the selected files.\nBased on these files, the book analyses the following subjects:\n Kernel Architecture Overview and Design Goals System Initializataion (What happens on system boot) System Calls (system_call and lcall7) Signals and Interrupts and Time Processes and Threads (Process Representation, Scheduling) Memory (Virtual Memory, Paging, Memory Mapping) IPC (Message Queues, Semaphores, Shared Memory) Symmetric Multiprocessing Tunable Kernel Parameters  Here\u0026rsquo;s an example view of the code presented in the book (file: include/asm-i386/siginfo.h).\nThe black arrow on the right side indicates the page to go for a more in-depth explanation. Here are the contents of the explanation pages:\nIt includes some introductory content about Interrupts and Signals and then it goes line-by-line to explain what the purpose of the code is, what structs it uses etc.\nConclusions Get both of them and at least have them in your bookcase. They are pretty cheap on \u0026lt;amazon.co.uk\u0026gt; and especially \u0026ldquo;Linux Kernel in a Nutshell\u0026rdquo; you can read through it in a day or so (obviously allocate more time if you want to follow along and explore the config parameters discussed). Regarding the \u0026ldquo;Core Kernel Commentary\u0026rdquo; I don\u0026rsquo;t expect to study it cover-to-cover, but it looks like a good resource for picking a particular subject and learning as much as possible about it.\n","permalink":"https://bergercookie.dev/post/linux-kernel-books/","summary":"I recently got my hands on two books that talk about the Linux Kernel. These are the Linux Kernel in a Nutshell, and LINUX Core Kernel Commentary.\nThese books are rather old, so some bits are outdated, but don\u0026rsquo;t let this fact discourage you. Both can be super useful and can definitely help you learn more about how the Kernel works.\nLinux Kernel In a Nutshell The first one is a classic.","title":"Linux Code Commentary ... in a nutshell"},{"content":"Easy2boot is a utility that allows you to flash a USB with multiple bootable images. This gives you the option of always carrying a variety of OSes / tools for all sorts of different scenarios (debugging using a Rescue image / gparted live, install Ubuntu / Windows 10 etc. images)\nHere are the contents of a E2B USB stick that I tend to carry with me and use on a regular basis.\nberger on draken in /me/berger/EASY2BOOT at [14:00:22] ‚ûú tree \\_ISO/LINUX/ \\_ISO/LINUX/ ‚îú‚îÄ‚îÄ android-x86_64-7.1-rc2.isodefault ‚îú‚îÄ‚îÄ archlinux-2017.12.01-x86_64.isodefault ‚îú‚îÄ‚îÄ gparted-live-1.1.0-1-amd64.isodefault ‚îú‚îÄ‚îÄ kali-linux-light-2018.4-amd64.isodefault ‚îú‚îÄ‚îÄ krd.isodefault ‚îú‚îÄ‚îÄ lxle_16.04.3_64.isodefault ‚îú‚îÄ‚îÄ TinyCore-current.isodefault ‚îú‚îÄ‚îÄ ubuntu-18.04.1.0-live-server-amd64.isodefault ‚îî‚îÄ‚îÄ ubuntu-18.04.1-desktop-amd64.isodefault  Setting up - Adding a Linux ISO  Download from here: https://www.fosshub.com/Easy2Boot.html Extract the Easy2Boot Linux archive - run the docs/linux_utils/fmt.sh script to format the USB device  Make sure you select the right drive. Bash script doesn\u0026rsquo;t look very well written and by default it picked my internal drive!   Copy your .iso files to MAINMENU or LINUX Rename the .iso to .isodefault Defrag the USB device:  sudo perl /media/berger/EASY2BOOT/\\_ISO/docs/linux_utils/defragfs /media/berger/EASY2BOOT/ -f Articles/Tutorials  Basic tutorial  Notes Try to use the same scripts for formatting / defragging that you used during the initial USB flashing. There are compabilitity issues between the different versions of the tool. If you don\u0026rsquo;t have the original downloaded zip / contents anymore, it might be worth it reflashing the USB and copying over the images to the newly created partition instead.\n","permalink":"https://bergercookie.dev/post/easy2boot/","summary":"Easy2boot is a utility that allows you to flash a USB with multiple bootable images. This gives you the option of always carrying a variety of OSes / tools for all sorts of different scenarios (debugging using a Rescue image / gparted live, install Ubuntu / Windows 10 etc. images)\nHere are the contents of a E2B USB stick that I tend to carry with me and use on a regular basis.","title":"Scratchpad - Create multi-ISO USBs with Easy2boot"},{"content":"Intro lsd is an alternative to the classic ls UNIX tool. It shows you a listing of files, directories, links, named pipes etc. The difference to its predecessor is that it makes use of Glyph-rich fonts such as Nerd Fonts as well as an abundance of colors preconfigured and available from start.\nTroubleshooting \u0026amp; How-to \u0026ldquo;Permission denied\u0026rdquo; when root or $LD_PRELOAD error on a gtk3-related library You\u0026rsquo;ve probably installed lsd via snap. Don\u0026rsquo;t; Either build it from source or install it with apt.\nUse lsd and redirect its output You might notice that when redirecting lsd output to e.g., a file, lsd detects that its output is not actually a terminal and turns off coloring and glyphs. Most of the time that\u0026rsquo;s the correct behavior, otherwise it would pollute the output file with escape characters for the colors. However if you still want to keep colors and glyphs on, here\u0026rsquo;s how you would do it\nls --tree --icon always --color always | less -r  # Or if piping to a file  ls --tree --icon always --color always | tee -a output-file ","permalink":"https://bergercookie.dev/post/lsd/","summary":"Intro lsd is an alternative to the classic ls UNIX tool. It shows you a listing of files, directories, links, named pipes etc. The difference to its predecessor is that it makes use of Glyph-rich fonts such as Nerd Fonts as well as an abundance of colors preconfigured and available from start.\nTroubleshooting \u0026amp; How-to \u0026ldquo;Permission denied\u0026rdquo; when root or $LD_PRELOAD error on a gtk3-related library You\u0026rsquo;ve probably installed lsd via snap.","title":"Scratchpad - LSD - A colorful ls alternative"},{"content":"I\u0026rsquo;ve lately been very active developing plugins for the Albert launcher for many parts of my daily routine. For those of you that don\u0026rsquo;t know it, Albert is an application launcher for Linux. It\u0026rsquo;s written in C++ and its GUI is built on top of Qt. The current post is a short summary of the plugins I\u0026rsquo;ve implemented to automate parts of my current routine and maximise my productivity along with advise and, hopefully, useful information on how to do the same for your needs via Albert.\nIf you want to extend Albert\u0026rsquo;s capabilities you basically have two options:\n Implement a native Albert plugin in C++. For this you have to clone the Albert repository, implement and compile your plugin along with the overall repo. Alternatively you can write a python extension. The latter can be developed separately from the main Albert repo and can be enabled/disabled via an option in the application settings. This goes without saying that, if you\u0026rsquo;re comfortable with Python and you\u0026rsquo;re willing to sacrifice a tiny bit of performance when the plugin is running, then this should be your go-to choice.  In my case, I followed the latter approach and in the last ~3 months I implemented a series of plugins to automate parts of my daily routine. All the plugins as well as the overall content of this article can be found in the awesome-albert-plugins repo. Following is a list of the plugins I implemented, along with a short description of what each plugin does:\n  Jira: Mark tickets as in-progress/under-code-review/done, do fuzzy search on their title, navigate to the corresponding ticket page\n  Taskwarrior: Interact with taskwarrior - fuzzy search on title, change status of task\n  Zoopla - Search Property to Buy, Rent, House Prices\n  Xkcd - Fetch xkcd comics - do fuzzy search on title\n  Google Maps - Fetch instructions from/to a specific place, optionally via specific transportation means\n  Suggestions-enabled search for various websites using googler. For example:\nGoogle Amazon Youtube Github Ebay IMDB \u0026hellip;\n  Here are some demo pictures of these plugins:\n| | | | | | | | |\n   Do a fuzzy search of your assigned JIRA tickets     View of many suggestions-enabled saerch plugins  Since I spent a good deal of time writing these plugins, I figured I could share some of my conclusions, shortcuts into writing a new plugin for your own usecases:\n  Use cookiecutter to minimise the boilerplate code for your new plugin. The former uses the Jinja templating language to substitute placeholder values in the contents of files and in the names of the files and directories. Here\u0026rsquo;s the cookiecutter package layout that I have been using for spawning new Albert plugins:\n cookiecutter/  ‚îú‚îÄ‚îÄ cookiecutter.json  ‚îú‚îÄ‚îÄ {{cookiecutter.plugin_name}}  ‚îÇ¬†‚îú‚îÄ‚îÄ __init__.py  ‚îÇ¬†‚îú‚îÄ‚îÄ install-plugin.sh  ‚îÇ¬†‚îú‚îÄ‚îÄ misc  ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ demo.gif  ‚îÇ¬†‚îî‚îÄ‚îÄ README.md  ‚îî‚îÄ‚îÄ README.md   And here are the modifiable variables when creating a new plugin:\n1 2 3 4 5 6 7 8 9 10 11   {  \u0026#34;author\u0026#34;: \u0026#34;Nikos Koukis\u0026#34;,  \u0026#34;plugin_name\u0026#34;: \u0026#34;albert_plugin\u0026#34;,  \u0026#34;parent_repo_url\u0026#34;: \u0026#34;https://github.com/bergercookie/awesome-albert-plugins\u0026#34;,  \u0026#34;repo_base_url\u0026#34;: \u0026#34;https://github.com/bergercookie/awesome-albert-plugins/blob/master/plugins/\u0026#34;,  \u0026#34;download_url_base\u0026#34;: \u0026#34;https://raw.githubusercontent.com/bergercookie/awesome-albert-plugins/master/plugins/{{ cookiecutter.plugin_name }}/\u0026#34;,  \u0026#34;plugin_short_description\u0026#34;: \u0026#34;Some title for your plugin\u0026#34;,  \u0026#34;albert_plugin_interface\u0026#34;: \u0026#34;v0.2\u0026#34;,  \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;  }       Give the user an easy way of raising issues and bugs\nWhen using an albert plugin (or a general-purpose python script) many things can go wrong at runtime (HTTP connection errors, filesystem errors etc.). Moreover, the potential plugin user may be running albert on the background, without having access to the corresponding pseudoterminal it was launched from. This is why, in case of an error, the plugin will silently fail. Here\u0026rsquo;s a snippet of code that I\u0026rsquo;ve been using so that errors are not left unnoticed as well as an easy way for the user to report the bug:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24   import traceback   if query.isTriggered:  try:  # whatever your pugin does goes here  # ...  except Exception: # user to report error  results.insert(  0,  v0.Item(  id=__prettyname__,  icon=icon_path,  text=\u0026#34;Something went wrong! Press [ENTER] to copy error and report it\u0026#34;,  actions=[  v0.ClipAction(  f\u0026#34;Copy error - report it to {__homepage__[8:]}\u0026#34;,  f\u0026#34;{traceback.format_exc()}\u0026#34;,  )  ],  ),  )   return results     Now, in case an exception is raised during runtime, the user is greeted with the following message:\nAnd when pressing ENTER, the appropriate ISSUES page URL is copied to the clipboard.\n  Have a setup phase for checking that all the requirements for your plugin are in place\nIn case more setup steps are required (e.g., login credentials) that can happen as part of the first albert plugin run(s). In my case, I\u0026rsquo;ve been using a setup function called at the start of the plugin that checks that all the requirements are in-place before actually going through the main plugin functionality. Here\u0026rsquo;s a sample of how the setup phase works for the JIRA plugin:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75   if query.isTriggered:  try:  results_setup = setup(query)  if results_setup:  return results_setup  # setup OK.   user = load_data(\u0026#34;user\u0026#34;)  server = load_data(\u0026#34;server\u0026#34;)  api_key = load_api_key()   ...   def setup():  results = []   if not shutil.which(\u0026#34;pass\u0026#34;):  results.append(  v0.Item(  id=__prettyname__,  icon=icon_path,  text=f\u0026#39;\u0026#34;pass\u0026#34; is not installed.\u0026#39;,  subtext=\u0026#39;Please install and configure \u0026#34;pass\u0026#34; accordingly.\u0026#39;,  actions=[  v0.UrlAction(\u0026#39;Open \u0026#34;pass\u0026#34; website\u0026#39;, \u0026#34;https://www.passwordstore.org/\u0026#34;)  ],  )  )  return results   # user  if not user_path.is_file():  results.append(  v0.Item(  id=__prettyname__,  icon=icon_path,  text=f\u0026#34;Please specify your email address for JIRA\u0026#34;,  subtext=\u0026#34;Fill and press [ENTER]\u0026#34;,  actions=[v0.FuncAction(\u0026#34;Save user\u0026#34;, lambda: save_data(query.string, \u0026#34;user\u0026#34;))],  )  )  return results   # password (same for API key)  if not server_path.is_file():  results.append(  v0.Item(  id=__prettyname__,  icon=icon_path,  text=f\u0026#34;Please specify the JIRA server to connect to\u0026#34;,  subtext=\u0026#34;Fill and press [ENTER]\u0026#34;,  actions=[  v0.FuncAction(  \u0026#34;Save JIRA server\u0026#34;, lambda: save_data(query.string, \u0026#34;server\u0026#34;)  )  ],  )  )  return results   # ...  def save_data(data: str, data_name: str):  \u0026#34;\u0026#34;\u0026#34;Save a piece of data in the configuration directory.\u0026#34;\u0026#34;\u0026#34;  with open(config_path / data_name, \u0026#34;w\u0026#34;) as f:  f.write(data)    def load_data(data_name) -\u0026gt; str:  \u0026#34;\u0026#34;\u0026#34;Load a piece of data from the configuration directory.\u0026#34;\u0026#34;\u0026#34;  with open(config_path / data_name, \u0026#34;r\u0026#34;) as f:  data = f.readline().strip().split()[0]   return data      Thus, in this case, on the fist run, the user will have to write their username and server on the albert prompt, which will be saved in appropriate files, and in the following 2 plugin runs, the plugin will check that their password and API key can be appropriately found using the Pass password manager.\n  Give the user an easy way of installing the plugin.\nAlong with each new albert plugin, I\u0026rsquo;ve been adding (again generating it via cookiecutter) an install-plugin.sh script that sets up all the plugin dependencies via either the system package manager (e.g., apt-get) or the language package manager (i.e., pip) and also copies the plugin directory to the local albert modules installation, i.e.: ~/.local/share/albert/org.albert.extension.python/.\n  That\u0026rsquo;s all for now. Let me know if this has been helpful or has given you any inspiration for making your own extensions.\n","permalink":"https://bergercookie.dev/post/albert-plugins/","summary":"I\u0026rsquo;ve lately been very active developing plugins for the Albert launcher for many parts of my daily routine. For those of you that don\u0026rsquo;t know it, Albert is an application launcher for Linux. It\u0026rsquo;s written in C++ and its GUI is built on top of Qt. The current post is a short summary of the plugins I\u0026rsquo;ve implemented to automate parts of my current routine and maximise my productivity along with advise and, hopefully, useful information on how to do the same for your needs via Albert.","title":"Using Albert to boost your productivity"},{"content":"Computer Vision Applications using OpenCV Facial Landmark Detection Improve the speed of Facial Landmark Detection:  Use face detection that comes with OS - if possible (e.g., Android, iOS) Skip frames - e.g,. run every 3 frames Downscale image, detect faces there \u0026hellip; then propagate bboxes to original image (dividing the coordinates by the scale used for resizing the original frame.). Then just do landmark detection on original image and computed bounding box  Improve landmark stabilisation  Weighted Moving Average - Average the landmark point location over a small time window Kalman Filtering Optical Flow  Code snippets OpenCV - Rescale an image - keep ratio import cv2  # Load image  img = cv2.imread(\u0026#34;\u0026lt;path-to-img\u0026gt;\u0026#34;)  # Scale down - keep ratio  dims = img.shape[0:2] ratio = dims[0] / dims[1]  target_x = 960 # resolution - x target_y = int(960 / ratio) imgS = cv2.resize(img, (target_x, target_y))  cv2.namedWindow(\u0026#39;windowname\u0026#39;, cv2.WINDOW_NORMAL) # Create window with freedom of dimensions cv2.imshow(\u0026#39;windowname\u0026#39;, imgS) cv2.waitKey(0) # Display the image infinitely until any keypress  # in case you want to kill the window  cv2.destroyAllWindows() OpenCV - imshow but keep it until pressed import cv2 import time  def imshow\\_(winname: str, img): cv2.imshow(winname, img)   while True:  k = cv2.waitKey()  if k == 27:  break  time.sleep(0.2)   cv2.destroyWindow(winname)() Hints \u0026amp; Tricks  dlib::faceDetector is trained on 80x80 faces. If the faces in your images are less than that then you need to upscale before running the detection. dlib::landmarkDetector: Paper says it runs in 1ms dlib -\u0026gt; RGB, OpenCV -\u0026gt; BGR. Thus for OpenCV -\u0026gt; dlib: cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  Useful Links  dlib OpenFace Demystifying Face Recognition: Face Alignment  So we were not able to confirm that using aligned images for model learned on cropped faces boost the accuracy.   OpenCV - Python filtering operations  ","permalink":"https://bergercookie.dev/post/opencv-applications-ii/","summary":"Computer Vision Applications using OpenCV Facial Landmark Detection Improve the speed of Facial Landmark Detection:  Use face detection that comes with OS - if possible (e.g., Android, iOS) Skip frames - e.g,. run every 3 frames Downscale image, detect faces there \u0026hellip; then propagate bboxes to original image (dividing the coordinates by the scale used for resizing the original frame.). Then just do landmark detection on original image and computed bounding box  Improve landmark stabilisation  Weighted Moving Average - Average the landmark point location over a small time window Kalman Filtering Optical Flow  Code snippets OpenCV - Rescale an image - keep ratio import cv2  # Load image  img = cv2.","title":"üõ†Ô∏è  Scratchpad - CV Applications II Course by OpenCV"},{"content":" Definition: The science/art of programming computers so that they can learn from data\n Popular ML Algorithms  Linear \u0026amp; Polynomial Regression Logistic Regression k-nearest Neighbors Support Vector machines Decision Trees Random Forests Ensemble methods  Neural Networks Architectures  Feedforward Neural Nets Convolutional Nets Recurrent Nets Long short-term memory (LSTM) nets Autoencoders Multi-Layer Perceptons (MLPs)  Famous Papers  Machine Learning on handwritten digits - 2006 - \u0026lt;www.cs.toronto.edu/~hinton\u0026gt; The Unreasonable Effectiveness of Data - 2009  Useful links - resources  www.kaggle.com/  Competitions Datasets Kernels   OpenAI Gym  For reinforcement learning   scikit-learn user guide Dataquest - www.dataquest.io deep learning website - http://deeplearning.net Imperial College Course  https://www.deeplearningmathematics.com/slides-materials/ https://github.com/pukkapies/dl-imperial-maths   Machine Learning - The Complete Guide https://en.wikipedia.org/wiki/Book:Machine_Learning_%E2%80%93_The_Complete_Guide https://paperswithcode.com/  Hands-On Machine Learning Book For the DL part see [Deep Learning]\nPandas / Sklearn / Numpy / Scipy Cheatsheet dt.describe() ‚Üí statistics about each column (count, mean, min, max 25% 50% etc.) dt.info() ‚Üí info about dataframe (dtype index, column dtypes, not-null values, memory usage) dt[\u0026quot;a_col\u0026quot;].value_counts() ‚Üí get all the values encountered in the column dt.corr() ‚Üí Compute standard correlation coefficient for potential linear correlations\nfrom pandas.plotting import scatter_matrix scatter_matrix(dt, figsize=(12,8)) Apply a function to a dataframe: either dt.apply or dt.where(... , inplace=True)\nUse the viridis color palette: color-blind-friendly and prints better on greyscale!\nhttps://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html\nSkLearn - fill missing values in a dataset: from sklearn.preprocessing import Imputer imputer = Imputer(strategy=\u0026#34;median\u0026#34;) imputer.fit(dt) # dt must have numerical values only Strategies:  Get rid of corresponding districts Get rid of the whole attribute Set the values to some value (zero, mean, median, etc.)  Pandas/SkLearn: Convert a string column/category to nums\ndt_encoded, dt_categories = dt.factorize()  # OR Use One-Hot Encoding - each string in the string category becomes a  # separate attribute  sklearn.preprocessing.OneHotEncoder Get Numpy dense array from Scipy sparse matrix: sparse_mat.toarray()\nFeature Scaling Machine Learning algorithms don\u0026rsquo;t perform well when the input numerical attributes have very different scales.\n min-max scaling / normalization standardization  Definitions Attribute: A data type (e.g., Mileage) Feature: Attribute + its value\nDeep Neural Network LTU: Neuron, a Sum using weights -\u0026gt; z = w1x1 + w2x2 + \u0026hellip; + wnxn (w^Tx), gives out a step function -\u0026gt; e.g., Heaviside\nPerceptron -\u0026gt; single Layer of LTUs, Each neuron is connected to all input. The enuron\u0026rsquo;s are also fed an extra bias feature x0 = 1 (bias neuron)\nPassthrough Input Layer: Inputs are represented by neurons that just propagate the input to the output\nActivation function (activation_fn): The function that evaluates the neuron inputs and dicides on the triggering of the neuron\nReLU or Rectifier or Ramp -\u0026gt; max(0, z)\nHint: The derivative of ReLU is the Heaviside and of the SmoothReLU the logistic function\nDeep Learning Theorems   Universal Approximation Theorem\n  No Free Lunch Theorem\nAny two optimization algorithms are equivalent when their performance is averaged across all possible problems\n  FAQ  How do I tune the hyperparameters of my model?  Grid search with cross-validation to find the right hyperparameters Randomised search Use Oscar - http://oscar.calldesk.ai/  It helps to have an idea of what values are reasonable for each hyperparameter!      ","permalink":"https://bergercookie.dev/post/machine-learning/","summary":"Definition: The science/art of programming computers so that they can learn from data\n Popular ML Algorithms  Linear \u0026amp; Polynomial Regression Logistic Regression k-nearest Neighbors Support Vector machines Decision Trees Random Forests Ensemble methods  Neural Networks Architectures  Feedforward Neural Nets Convolutional Nets Recurrent Nets Long short-term memory (LSTM) nets Autoencoders Multi-Layer Perceptons (MLPs)  Famous Papers  Machine Learning on handwritten digits - 2006 - \u0026lt;www.cs.toronto.edu/~hinton\u0026gt; The Unreasonable Effectiveness of Data - 2009  Useful links - resources  www.","title":"üõ†Ô∏è  Scratchpad - Machine Learning"},{"content":"A scratchpad for common pitfalls, commands, and shortcuts when using ROS1.\nQuestions/Answers Set an argument based on the exclusive OR of two other arguments This is a really ugly hack, but it gets the job done. Since we don\u0026rsquo;t care much about the name, we set it so that if the user provides both of them at the same time, they get a descriptive message back.\n\u0026lt;!-- arg1 XOR arg2 --\u0026gt; \u0026lt;arg if=\u0026#34;$(eval (arg1 != \u0026#39;\u0026#39;) == (arg2 != \u0026#39;\u0026#39;))\u0026#34; name=\u0026#34;Please_Set_Exclusively_Either_Arg1_Or_Arg2\u0026#34;/\u0026gt; \u0026lt;arg if=\u0026#34;$(eval (arg1 != \u0026#39;\u0026#39;) == (arg2 != \u0026#39;\u0026#39;))\u0026#34; name=\u0026#34;dummy\u0026#34; value=\u0026#34;$(arg Please_Set_Exclusively_Either_Arg1_Or_Arg2)\u0026#34;/\u0026gt; Set a parameter based on an environment variable, fallback to a default \u0026lt;launch\u0026gt; \u0026lt;arg name=\u0026#34;mydefault\u0026#34; value=\u0026#34;kalimera\u0026#34;/\u0026gt; \u0026lt;arg if=\u0026#34;$(eval optenv(\u0026#39;KALIMERA\u0026#39;) != \u0026#39;\u0026#39;)\u0026#34; name=\u0026#34;kalimera\u0026#34; value=\u0026#34;$(env KALIMERA)\u0026#34;/\u0026gt; \u0026lt;arg unless=\u0026#34;$(eval optenv(\u0026#39;KALIMERA\u0026#39;) != \u0026#39;\u0026#39;)\u0026#34; name=\u0026#34;kalimera\u0026#34; value=\u0026#34;$(arg mydefault)\u0026#34;/\u0026gt;  \u0026lt;param name=\u0026#34;kalimera\u0026#34; value=\u0026#34;$(arg kalimera)\u0026#34;/\u0026gt; \u0026lt;/launch\u0026gt; How to debug launchfile execution See the following CLI arguments\n--wait Delay the launch until a roscore is detected.  --local Launch of the local nodes only. Nodes on remote machines will not be run.  --screen Force all node output to screen. Useful for node debugging.  --log Force all node output to log file. Also useful for node debugging.  -v Enable verbose printing. Useful for tracing roslaunch file parsing.  --dump-params Print parameters in launch file in YAML format. Roslaunch conditional  \u0026lt;include file=\u0026#34;...\u0026#34; \u0026gt; \u0026lt;arg if=\u0026#34;$(arg var1)\u0026#34; name=\u0026#34;var\u0026#34; value=\u0026#34;value\u0026#34; /\u0026gt; \u0026lt;arg unless=\u0026#34;$(arg var1)\u0026#34; name=\u0026#34;var\u0026#34; value=\u0026#34;value2\u0026#34; /\u0026gt; \u0026lt;/include\u0026gt; Visualise topics/services/images etc from the command line (without X/OpenGL): Use rosshow\nKill ros/gazebo related processes ps -ef | grep -E ros\\|melodic | awk \u0026#39;{print 2}\u0026#39; | xargs kill -9 How do I debug a URDF file and its transforms? Use the urdfdom tools (independent package)\napt-get install liburdfdom-tools graphviz urdf_to_graphiz \u0026lt;path-to-your-urdf You can also use the xacro package and the check_urdf tool:\nrosrun xacro xacro.py `rospack find pr2_description`/robots/pr2.urdf.xacro -o /tmp/pr2.urdf check_urdf pr2.urdf [ERROR]: Creation of publisher failed: unknown error handler name \u0026lsquo;rosmsg\u0026rsquo; There is a bug in genpy for version \u0026lt;= 0.6.13, try apt-get upgrade-ing it\nSource\nError: RQT doesn't list plugins on startup: Remove its cache, then it works: rm ~/.config/ros.org/rqt_gui.ini\nSource\nI\u0026rsquo;m getting the following error: multiple files named ... in package Disable install space.\ncatkin clean catkin build \u0026lt;package-name\u0026gt; --no-install Run catkin for the package in the current directory: catkin build --this -DCMAKE... Publish to /move_base_simple/goal: rostpic pub /move_base_simple/goal geometry_msgs/PoseStamped \u0026#39;{header: {stamp: now, frame_id: \u0026#34;map\u0026#34;}, pose: {position: {x: 1.0, y: 0.0, z: 0.0}, orientation: {w: 1.0}}}\u0026#39; Source\nimage_transport plugins - how to setup: sudo apt-get install ros-melodic-\\*-image-transport rosrun image_transport republish compressed /in/compressed:=/\u0026lt;path-to-topic\u0026gt;/compressed_image0 \u0026#34;raw\u0026#34; out:=/\u0026lt;path-to-topic\u0026gt;/image0 Source\nUse ninja when building with catkin_make catkin_make --use-ninja --cmake-args -DCMAKE_BUILD_TYPE=Release Adjust logger verbosity - inspect \u0026ldquo;debug\u0026rdquo; messages: Run the rqt_logger_level GUI:\nrosrun rqt_logger_level rqt_logger_level Alternatively adjust it using the service call:\nrosservice list rosservice call /rviz_123/get_loggers \u0026lt;tab\u0026gt;\u0026lt;tab\u0026gt; rosservice call /rviz_123/set_logger_level \u0026lt;tab\u0026gt;\u0026lt;tab\u0026gt; Adjust the logger verbosity from the start of the run Define your own ROSCONSOLE_CONFIG_FILE variable + config file.\nrosconsole will load a config file from $ROS_ROOT/config/rosconsole.config when it initializes.\nrosconsole also lets you define your own configuration file that will be used by log4cxx, defined by the ROSCONSOLE_CONFIG_FILE environment variable. Anything defined in this config file will override the default config file.\nA simple example:\n# Set the default ros output to warning and higher log4j.logger.ros=WARN # Override my package to output everything log4j.logger.ros.my_package_name=DEBUG List all available plugins of a particular package rospack plugins --attrib=plugin nav_core See also: http://wiki.ros.org/pluginlib\nHave detailed output for debugging # Try one of the following  export ROSCONSOLE_FORMAT=\u0026#39;[${severity}] [${time}]: ${message}\u0026#39; # default export ROSCONSOLE_FORMAT=\u0026#39;${severity} | ${time} | ${message}\u0026#39; export ROSCONSOLE_FORMAT=\u0026#39;${severity} | ${node} | ${time} | ${message} | ${file}:${line}\u0026#39; export ROSCONSOLE_FORMAT=\u0026#39;${severity} | ${node} - ${thread} | ${time} | ${message} | +${line} ${file}\u0026#39; ROS Unittesting If you use only gtest then you have to add your target like this:\ncatkin*add_gtest(UT*${PROJECT_NAME} test/test_file.cpp ...) However, if you also use gmock then you should use catkin_add_gmock instead!\ncatkin*add_gmock(UT*{PROJECT_NAME} test/test_file.cpp ...) To run all the tests:\ncatkin_make run_tests Notice that the previous call will return a 0 (success) error code in any case even if the tests fail.\nTo get a summary and get the appropriate error code you can either run catkin_test_results or the CTest target test:\ncatkin_make test Source: https://github.com/ros/catkin/issues/576\nGet all ROS topics programmatically Query the master; from C++ use something like this:\n#include \u0026lt;ros/master.h\u0026gt;// see /opt/ros/\u0026lt;ros-version\u0026gt;/include/ros/master.h for more details on this // struct ros::master::V_TopicInfo allTopics; ros::master::getTopics(allTopics); Source\nRemap a topic in the same TF Tree \u0026lt;node name=\u0026#34;remapper\u0026#34; pkg=\u0026#34;tf_remapper_cpp\u0026#34; type=\u0026#34;tf_remap\u0026#34;\u0026gt; \u0026lt;rosparam param=\u0026#34;mappings\u0026#34;\u0026gt;[{old: /slamcore/map, new: /kalimera}]\u0026lt;/rosparam\u0026gt;   \u0026lt;param name=\u0026#34;old_tf_topic_name\u0026#34; value=\u0026#34;/tf\u0026#34; /\u0026gt;  \u0026lt;param name=\u0026#34;new_tf_topic_name\u0026#34; value=\u0026#34;/tf\u0026#34; /\u0026gt; \u0026lt;/node\u0026gt; More Useful links  Environment variables ROS1 Turtlebot navigation tutorial  REPs of interest  REP stands for ROS Enhancement Proposal. A REP is a design document providing information to the ROS community, or describing a new feature for ROS or its processes or environment.\n  REP-117:  Readings too close to measure -\u0026gt; -Inf Invalid measurements -\u0026gt;  NaN Readings of no return -\u0026gt; +Inf   REP-118  Representing depth data Use 32-bit Float   REP-105  Frames of reference convention Relevant answer with rationale    ROS Ecosystem - Miscellaneous Tools  champ: ROS packages for Quadruped Robot based on MIT Cheetah I colcon: CLI Tool to improve the workflow of building, testing, and using multiple packages - Default build orchestrator for ROS2 vcstool: VCS Designed to facilitate working with multiple repositories publish-python: Python script to publish your python code to Github Release / PyPI, etc. or generate a debian package  TF Tree - Precision It seems that the transforms published in the TF tree (both in ROS1 and ROS2) are accurate up to millimetres. This means that If I publish a transform translation like [0.123456789,0.0,0.0] it will appear like [0.123,0.0,0.0] when I read it using tf tf_echo or tf2_ros tf2_echo from the command line.\nA Mental Model of the ROS1 Navigation Stack  ","permalink":"https://bergercookie.dev/post/ros-faq/","summary":"A scratchpad for common pitfalls, commands, and shortcuts when using ROS1.\nQuestions/Answers Set an argument based on the exclusive OR of two other arguments This is a really ugly hack, but it gets the job done. Since we don\u0026rsquo;t care much about the name, we set it so that if the user provides both of them at the same time, they get a descriptive message back.\n\u0026lt;!-- arg1 XOR arg2 --\u0026gt; \u0026lt;arg if=\u0026#34;$(eval (arg1 !","title":"üõ†Ô∏è  Scratchpad - ROS1"},{"content":"As this is my first actual post I thought I\u0026rsquo;d write about something I\u0026rsquo;m a big fun of, tooling. More specifically I want to describe my workflow when working or in general when using my machine.\nMy general goal is to be productive in my day-to-day routine, and automate everything that can actually be automated so that I can focus on the higher-level tasks I\u0026rsquo;m interested in. I admit that sometimes I\u0026rsquo;m overdoing it and automate a task that I don\u0026rsquo;t actually have to but all in all I think this attitude has helped me considerably so far.\nI\u0026rsquo;m also a big believer in open-source. I\u0026rsquo;m striving to publish all my coding projects on Github (see my pinned repos for some of the works I\u0026rsquo;m proud of) and I\u0026rsquo;ve also been mentoring for the past two years for the Google Summer of Code Project with the MRPT robotics organisation. Thus, I\u0026rsquo;m trying as much as possible to use open-source alternatives for all my tasks. That being said, if there is a paid-for tool that does something great I\u0026rsquo;m happy to pay for it (e.g., see the excellent C/C++ Code Explorer - Sourcetrail).\nHere\u0026rsquo;s a bullet list of the software I\u0026rsquo;m using:\n OS: GNU/Linux Linux flavour: Ubuntu/Debian Editor: Vim/Neovim Browser: Firefox Terminal: Alacritty + Tmux multiplexer + Bash Desktop Environment: i3  Let me now elaborate on the bullets above.\nGNU/Linux - Debian-flavours Linux is free, as in freedom, it\u0026rsquo;s ubiquitous and is simply the bestest operating system there is üòè. Regarding flavour, I simply use Debian or Ubuntu as the main operating system just because I feel most comfortable in these systems It\u0026rsquo;s nice know how to open all sorts of files (xdg-open/xdg-mime) search for package names (apt-cache) install packages / download sources (apt-cache, apt-get, dpkg) and in general know how your system works. Also being a roboticist by profession, there is a strong inclination towards Debian / Ubuntu since ROS (1) ships binaries for specifically for these platforms and it\u0026rsquo;s a total PITA to compile it from source. Finally I consider it a big deal to always have a stable system even if that means not compiling your project with the latest and greatest gcc-8 compiler. Ubuntu and Debian give you that and also provide ways of installing more recent versions (linuxbrew, Ubuntu PPAs) and hey, you can always compile stuff from source if the previous don\u0026rsquo;t work for you üòâ\nVim/Neovim There is a gazillion of articles on how to use vim or what to put in your vimrc so I\u0026rsquo;m going to keep it short here.\nHere are some of the plugins that I use on a daily basis and I\u0026rsquo;m confident have boosted my editing efficiency significantly. Refer to the corresponding READMEs for more details:\n Plugin manager: vim-plug Asynchronous syntax checking/linting: ale Semantically-accurate syntax highlighting: chromatica Indentation level detection: detectindent Git client: fugitive Printf-like code debugging (2): debugstring UNIX helper functions: eunuch Asynchronous fuzzy searching: fzf Personal knowledge base /Task management: vimwiki, taskwiki  The important thing to note here is that you have to find a combination that works for you.\n Don\u0026rsquo;t use more plugins than you actually need or you\u0026rsquo;ll bloat vim and you\u0026rsquo;ll have awful startup times, Double-check that adding the plugin is worth the maintenance cost / potential startup overhead / effort to learn it. Maybe there\u0026rsquo;s already has a way of expressing it? Maybe it\u0026rsquo;s just a matter of a single function in your .vimrc?  Finally, here\u0026rsquo;s a link to my vim configuration in case you want to take a better look: vim-dotfiles\nFirefox Firefox is the de-facto open-source internet browser. It\u0026rsquo;s fast, it\u0026rsquo;s sleek and using the new Firefox WebExtensions and tridactyl one can browse the web like it\u0026rsquo;s Vim (3).\nAlacritty/Tmux Alacritty represents a newer trend in terminal emulators in that its objective is to render the terminal output using the computer GPU. Other than that, its developers strive to keep it minimal by not implementing additional features such as preferences window GUI (everything is managed by a YAML file) multiple tabs, split panes etc and instead keep it simple and fast. For most of the functionality that\u0026rsquo;s omitted (e.g., split panes), they advice the usage of a tool designed for that such as \u0026hellip; ü•Å Tmux!\nTmux is the modern alternative to the good-old screen multiplexer. The big difference compared to its predecessor is the good list of plugins and customisation options it gives to its user. It comes with its own plugin manager tpm along with plugins such as tmux-yank, tmux-continuum , and tmux-resurrect which significantly enhance the Tmux user experience.\nFinally, I\u0026rsquo;m using Bash for general navigation, system-interaction tasks. Bash is the default Linux shell, and 99% of Linux machines you encounter will have it. Yes, I am aware of zsh and the billion of plugins that it offers. In general it is a superior shell compared to Bash but honestly the difference is not big enough to counter for the fact that bash is ubiquitous and eventually you\u0026rsquo;ll be forced to used it when working on a new machine / someone else\u0026rsquo;s laptop.\nHere\u0026rsquo;s my terminal configuration at the time of writing this:\ni3 i3 is a tiling window manager, meaning that it\u0026rsquo;s designed for placing windows as individual tiles in your screen(s). Since Tmux does most of the heavy-lifting in the terminal (splitting to panes, multiple sessions, multiple named windows etc.). i3 gives you (almost) the configurability of a window manager such as awesome but with much less hassle for configuring it to reach a version that works.\nI use i3 mainly for the following:\n  Split apps into workspaces. Be able to navigate between workspaces and between apps using only the mouse\n  Stacked views of windows\n  Automatic placement of a windowed app on startup/launch.\nNo more shoving windows around when you launch a program. It will automatically take its place based on the corresponding workspace. You can do it with simple rules in your i3 config file. For example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18   # assign apps to workspaces   assign [class=\u0026#34;Firefox\u0026#34;] $ws_www  assign [class=\u0026#34;Terminator\u0026#34;] $ws_term  assign [class=\u0026#34;Alacritty\u0026#34;] $ws_term  assign [class=\u0026#34;Franz\u0026#34;] $ws_msg  assign [class=\u0026#34;Viber\u0026#34;] $ws_msg  assign [class=\u0026#34;Slack\u0026#34;] $ws_msg  assign [class=\u0026#34;calibre\u0026#34;] $ws_calibre  assign [class=\u0026#34;spotify\u0026#34;] $ws_music  assign [class=\u0026#34;Gnome-terminal\u0026#34;] $ws_fm  assign [class=\u0026#34;Transmission-gtk\u0026#34;] $ws_random  assign [class=\u0026#34;Filezilla\u0026#34;] $ws_random  assign [class=\u0026#34;Nautilus\u0026#34;] $ws_fm  assign [class=\u0026#34;Pcmanfm\u0026#34;] $ws_fm  assign [class=\u0026#34;Vmplayer\u0026#34;] $ws_win  assign [class=\u0026#34;vlc\u0026#34;] $ws_www       Support for beautiful dock with reasonable configuration and time spent, via the use of powerline symbols + i3status-rust. Here is a sample:\n  Miscellaneous UNIX-related tooling This is a list of tools that didn\u0026rsquo;t fit any of the previous categories but are still worth mentioning:\n  Password management: UNIX Password Pass\nI can\u0026rsquo;t emphasise how easy and intuitive password management has been it since I came across Pass. It\u0026rsquo;s a minimal tool built on top of well established software such as GPG (for encryption) and Git for version control. There is also a fully functional android app PasswordStore that syncs via git so you can have all your passwords in all your devices and actually reason on how the whole pipeline actually works.\n  E-Book/Scientific works management: Calibre\nCalibre is an open-source e-book management tool written in Python. Apart from its obvious task, it can sync your books with Kindle or android devices it can import and manage various formats (e.g., pdf, mobi), and it can also has a pretty decent android client.\n  Personal TODO list - task management: Taskwarrior\nMore on this in another post.\n  Linux application Launcher: Albert\n  Fuzzy searching, autocompletion, directory navigation: fzf\n  Rust-alternatives to classic UNIX tools: ripgrep, fd\n  Messeging apps bundler: Franz\n  More on the menu - feedback As this is my first post, feel free to add some feedback in the comments and let me know what you think of this.\n  Don\u0026rsquo;t know what ROS is? Google it or wait for me to write an article on its latest version ROS2.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Disclaimer: I am the author of vim-debugstring\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I know that Chrome also offers vim-like binding plugins but from my experience with them there\u0026rsquo;s just a world of difference between those and Firefox plugins such as Tridactyl and its predecessors Vimperator / Pentadactyl.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://bergercookie.dev/post/daily-workflow/","summary":"As this is my first actual post I thought I\u0026rsquo;d write about something I\u0026rsquo;m a big fun of, tooling. More specifically I want to describe my workflow when working or in general when using my machine.\nMy general goal is to be productive in my day-to-day routine, and automate everything that can actually be automated so that I can focus on the higher-level tasks I\u0026rsquo;m interested in. I admit that sometimes I\u0026rsquo;m overdoing it and automate a task that I don\u0026rsquo;t actually have to but all in all I think this attitude has helped me considerably so far.","title":"My daily workflow and tools"},{"content":"Hey people, this is the first post in my attempt of a proper Jekyll blog.\nThroughout the, not that frequent, posts of mine, I\u0026rsquo;ll be writing about interesting technical stuff; These would be C++, Python, Vim, robotics, Linux (yes, that broad üòä )\nOff we go!\n","permalink":"https://bergercookie.dev/post/hello-world/","summary":"Hey people, this is the first post in my attempt of a proper Jekyll blog.\nThroughout the, not that frequent, posts of mine, I\u0026rsquo;ll be writing about interesting technical stuff; These would be C++, Python, Vim, robotics, Linux (yes, that broad üòä )\nOff we go!","title":"Hello world!"},{"content":"Taskwarrior for Google Tasks Sync  The current document describes the Taskwarrior \u0026lt;\u0026gt; GTasks Sync synchronization of syncall. To get the most updated version of this document, head over to the version in the repository\n See also the privacy policy for this app\nDescription Given all tasks in your Google Task task list and the task list of a Taskwarrior filter (combination of tags and projects) synchronise all the addition / modification / deletion events between them.\nDemo - populating a list in Google Tasks (view from Google Calendar) Motivation While Taskwarrior is an excellent tool when it comes to keeping TODO lists, keeping track of project goals etc., lacks the portability, simplicity and minimalistic design of Google Tasks. The latter also has the following advantages:\n Automatic sync across all your devices Comfortable addition/modification of events using voice commands Actual reminding of events with a variety of mechanisms  Override Google Tasks API key At the moment the Google Console app that makes use of the Google Tasks API is still in Testing mode and awaiting approval from Google. This means that if it raches more than 100 users, the integration may stop working for you. In that case in order to use this integration you will have to register for your own developer account with the Google Tasks API with the following steps:\nFirstly, removed the ~/.gtasks_credentials.pickle file on your system since that will be reused if found by the app.\nFor creating your own Google Cloud Developer App:\n Go to the Google Cloud developer console Make a new project From the sidebar go to API \u0026amp; Services and once there click the ENABLE APIS AND SERVICES button Look for and Enable the Tasks API  Your newly created app now has access to the Tasks API. We now have to create and download the credentials:\n  Again, from the sidebar under API And Services click Credentials\n  In the Google Tasks API screen, click the CREATE CREDENTIALS button.\n  Select the User data radio button (not the Application data).\n  Fill in the OAuth Consent Screen information (shouldn\u0026rsquo;t affect the process)\n  Allow the said credentials to access the following scopes:\n Create, edit, organize, and delete all your tasks View your tasks    Create a new OAuth Client ID. Set the type to Desktop App (app name is not important).\n  Finally download the credentials in JSON form by clicking the download button as shown below. This is the file you need to point to when running tw_gtasks_sync.\n  To specify your custom credentials JSON file use the --google-secret flag as follows:\ntw_gtasks_sync -l \u0026#34;\u0026lt;list-name\u0026gt;\u0026#34; -t \u0026#34;\u0026lt;taskwarrior-tag\u0026gt;\u0026#34; --google-secret \u0026#34;\u0026lt;path/to/downloaded/json/file\u0026gt;\u0026#34; Usage Examples Run the tw_gtasks_sync to synchronise the Google Tasks list of your choice with the selected Taskwarrior tag(s). Run with --help for the list of options.\n# Sync the +remindme Taskwarrior tag with the Google Tasks list named \u0026#34;TW Reminders\u0026#34;  tw_gtasks_sync --help tw_gtasks_sync -t remindme -l \u0026#34;TW Reminders\u0026#34; Installation Package Installation Install the syncall package from PyPI, enabling the google and tw extras:\npip3 install syncall[google,tw] Notes re this synchronization  Currently subtasks of a Google Tasks item are treated as completely independent of the parent task when converted to Taskwarrior It\u0026rsquo;s not possible to get the time part of the \u0026ldquo;due\u0026rdquo; field of a task using the Google Tasks API. Due to this restriction we currently do currently do sync the date part (without the time) from Google Tasks to Taskwarrior, but in order not to remove the time part when doing the inverse synchronization, we don\u0026rsquo;t sync the date at all from Taskwarrior to Google Tasks. More information in this ticket  ","permalink":"https://bergercookie.dev/projects/tw-gtasks/","summary":"Taskwarrior for Google Tasks Sync  The current document describes the Taskwarrior \u0026lt;\u0026gt; GTasks Sync synchronization of syncall. To get the most updated version of this document, head over to the version in the repository\n See also the privacy policy for this app\nDescription Given all tasks in your Google Task task list and the task list of a Taskwarrior filter (combination of tags and projects) synchronise all the addition / modification / deletion events between them.","title":""},{"content":"    resume.pdf    resume code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;}  body { font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; max-width: 800px; margin: auto; background: #FFFFFF; padding: 10px 10px 10px 10px; } h1 { font-size: 55px; color: #757575; text-align:center; margin-bottom:15px; } h2 { color: #397249; } h2:before { content: \"\"; display: inline-block; margin-right:1%; width: 16%; height: 10px; background-color: #9CB770; } dt { float: left; clear: left; width: 17%; font-weight: bold; } dd { margin-left: 17%; margin-bottom:7px; } p { margin-top:0; margin-bottom:7px; } blockquote { text-align: center } a { text-decoration: none; color: #397249; } a:hover, a:active { background-color: #397249; color: #FFFFFF; text-decoration: none; text-shadow: 1px 1px 1px #333; } hr { color: #A6A6A6; } table { width: 100%; }    Nikos Koukis  nickkouk@gmail.com ‚Ä¢ bergercookie.dev\nGithub://bergercookie ‚Ä¢ LinkedIn://nikos-koukis ‚Ä¢ Stackoverflow://bergercookie\nLondon - United Kingdom\n  I am a passionate Robotics/SLAM Engineer based in London. I love writing code and especially when that code comes into life in actual robots and real-life applications.\n Professional Experience  05/2019 - Robotics Product Engineer - SLAMcore  01/2019 - 05/2019 Robotics Engineer - SLAMcore  09/2017 - 01/2019 Junior Robotics Engineer - SLAMcore\nI work as a robotics product engineer at SLAMcore. We strive to provide robust and accurate SLAM solutions in Robotics. During my time there I have lead the integration and deployment of our software on multiple robotic platforms, and I have also worked in areas like sensor calibration, autonomous navigation, SLAM algorithms development, continuous integration as well as overall product development and deployment\n 02/2021 - 07/2021 Software Developer - Hellenic Army (KEPYES)\nAs part of my mandatory military service I maintained and implemented new features in large-scale Java and OracleSQL-based server applications. I also was the primary maintainer of legacy Linux-based servers essential for production apps. Create documentation and usage instructions for core components and tools including SVN, Git and Linux.\n 2017, 2018 Mentor at Google Summer of Code (GSoC) with MRPT  2016 Student at Google Summer of Code (GSoC) with MRPT\nDeveloped an open source implementation of the pose-graphSLAM algorithm with loop closure capabilities (Project link)\n  Technical Experience  MRPT 2016 - Core contributor at Mobile Robot Programming Toolkit (MRPT)\nMRPT is a open source robotics framework specialized in SLAM and mobile robot applications with over 300+ cites in Google Scholar, 40k+ downloads.\nI am the author and maintainer of the single and multi-robot implementations of mrpt-graphslam:\n mrpt-graphslam mrpt_graphslam_2d   Languages   C++ Very experienced using modern C++ (11, 14, 17 standards) and in working with popular mathematical / computer vision and robotics libraries such as OpenCV, Eigen, MRPT, OpenGV. I have also extensively developed applications in ROS, ROS2 and have used the Gazebo and V-REP robotic simulators.\nSample projects: MRPT, mrpt_slam, robot-concepts\n Python Expert in using either Python2 or Python3 and with using standard modules such as Numpy, Scipy, Pandas. Good knowledge of module such as argparse, click, pyyaml, mechanize. Decent knowledge of scikit-learn, Tensorflow.\nSample projects: taskw_gcal_sync, awesome_albert_plugins, mendeley2calibre, Pump3000\n Vim/Vimscript Implemented the vim-debugstring plugin for printf-like debugging in a variety of programming languages.  Rust I have been experimenting with Robotics/SLAM-related projects in the Rust programming language.    Excellent: Docker/Docker-compose, Make, Bash, Modern CMake, Git, Sed/Grep/Awk  Good: C, Fortran, Matlab  Basic: Haskell, Awk, Java, Ansible, Django, Grafana    Software: MRPT, ROS, ROS2, Gazebo, V-REP, Matlab, Fusion360, Solidworks, Grafana   Education  2011 - 2017 5yr Diploma in Mechanical Engineering\nNational Technical University of Athens (Athens, Greece)\nMaster Thesis: Design and Development of Single and Multi-Robot Simultaneous Localization and Mapping (SLAM) Algorithms\n8.4/10.0\n 2015 ERASMUS Studies\nKTH Royal Institute of Technology (Stockholm, Sweden)\nI studied for a semester in the department of Engineering Science where I undertook projects in advanced control theory, digital control, optimal control, and embedded systems for applications in robotics and aircraft control systems\n 2013 - Coursera/Udacity/EdX courses\nI have successfully completed more than 10 courses in various MOOC platforms including Udacity - Artificial Intelligence for Robotics, Udacity - Control of Mobile Robots, Coursera - Computer Networks.   Supplementary  Penetration Testing Enthusiast Languages:\n Greek (native speaker) English German (basic) Spanish (basic)  2014: 4th place in EBEC competition final round 2004: Avlonarion chess tournament champion\n     ","permalink":"https://bergercookie.dev/about/resume-view/","summary":"resume.pdf    resume code{white-space: pre-wrap;} span.smallcaps{font-variant: small-caps;} span.underline{text-decoration: underline;} div.column{display: inline-block; vertical-align: top; width: 50%;}  body { font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; max-width: 800px; margin: auto; background: #FFFFFF; padding: 10px 10px 10px 10px; } h1 { font-size: 55px; color: #757575; text-align:center; margin-bottom:15px; } h2 { color: #397249; } h2:before { content: \"\"; display: inline-block; margin-right:1%; width: 16%; height: 10px; background-color: #9CB770; } dt { float: left; clear: left; width: 17%; font-weight: bold; } dd { margin-left: 17%; margin-bottom:7px; } p { margin-top:0; margin-bottom:7px; } blockquote { text-align: center } a { text-decoration: none; color: #397249; } a:hover, a:active { background-color: #397249; color: #FFFFFF; text-decoration: none; text-shadow: 1px 1px 1px #333; } hr { color: #A6A6A6; } table { width: 100%; }    Nikos Koukis  nickkouk@gmail.","title":"Curriculum Vitae"}]